1. Initiated Git repository by: - 

git Init
git clone
git add remote origin

Then to push the main code: - 

git status 
git add .
git commit -m ""
git push -u origin  (this is it, nothing more in this line)

Then created conda environment using: - "conda create -p venv python==3.7.1 -y"


2. Setup the setup.py file: - 

imported 2 modules from setuptools library.  
find_packages() → Automatically searches your project directory and 
collects all valid Python packages, so you don’t have to list them manually.

setup() → The main function from setuptools that defines your package’s 
metadata (name, version, author, dependencies, etc.) and instructions for how it should be built/installed.

Then made a function to import packages names in a list (-e . there refers the txt file package names to setup.py file).
Then used command "pip install -r req.txt" to execute, cause -e . will take us to setup.py file.
Then defined Exception and logging


3. Then i developed code for ETL.

I took my csv, performed proper transformations and then convrted to json file format. 
Then i pushed them to mongo db in push_data.py file.
I had to go to mongo db and add ip (in cluster 0 part). Then i was able to push through.

4. I made ingestion Configuration

I had a folder named "constants". There i made a folder called "training_pipeline".
There, inside, i created an __init__.py file.
This will serve the purpose of holding all the constants within the code.
First of all, i included data ingestion constants. They are some folder names and file names.
Some general constants such as train_test_split_ratio also is there, along with mongodb addresses of my data.

In order to connect all this to my data ingestion code, i had another folder called "entity".
This folder served the purpose of all the configuration of the my entity and artifact(output fro a step).
Here, i joined paths with files, folders with folders. All this is done to give data_ingestion reference to constants
along with their referenced place in dir.

5. Data ingestion

~ We imported entity_config and training_pipeline dir __init__ file. Both of them have all the required constants.

    In the DataIngestion class, i
~ Imported all data from MongoDB and converted it into pandas dataframe. (removed the '_id' column.)

~ I saved that raw(feature) file inside the specified path.

~ Used train-test-split to split the data into 2 smaller files and saving them.

~ Final method included calling all of these methods and execute all this code.

I made 'artificat config'; where i specified the information i need out of data ingestion, for the data_validation part.

This was the path of the saved train and test files as string.

6. main.py

outside the Retail folder, i made another file with the name main.py,
i imported all the relevant libraries there and wrote a code to execute the ingestion pipeline. 

If this checked out properly. I went to next step, which was to create data_validation.




