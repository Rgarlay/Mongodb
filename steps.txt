1. Initiated Git repository by: - 

git Init
git clone
git add remote origin

Then to push the main code: - 

git status 
git add .
git commit -m ""
git push -u origin  (this is it, nothing more in this line)

Then created conda environment using: - "conda create -p venv python==3.7.1 -y"


2. Setup the setup.py file: - 

imported 2 modules from setuptools library.  
find_packages() → Automatically searches your project directory and 
collects all valid Python packages, so you don’t have to list them manually.

setup() → The main function from setuptools that defines your package’s 
metadata (name, version, author, dependencies, etc.) and instructions for how it should be built/installed.

Then made a function to import packages names in a list (-e . there refers the txt file package names to setup.py file).
Then used command "pip install -r req.txt" to execute, cause -e . will take us to setup.py file.
Then defined Exception and logging


3. Then i developed code for ETL.

I took my csv, performed proper transformations and then convrted to json file format. 
Then i pushed them to mongo db in push_data.py file.
I had to go to mongo db and add ip (in cluster 0 part). Then i was able to push through.

4. I made ingestion Configuration

I had a folder named "constants". There i made a folder called "training_pipeline".
There, inside, i created an __init__.py file.
This will serve the purpose of holding all the constants within the code.
First of all, i included data ingestion constants. They are some folder names and file names.
Some general constants such as train_test_split_ratio also is there, along with mongodb addresses of my data.

In order to connect all this to my data ingestion code, i had another folder called "entity".
This folder served the purpose of all the configuration of the my entity and artifact(output fro a step).
Here, i joined paths with files, folders with folders. All this is done to give data_ingestion reference to constants
along with their referenced place in dir.

5. I made Data ingestion

~ Imported all data from MongoDB and converted it into pandas dataframe. (removed the '_id' column.)

~ I saved that raw file inside the specified path.

~ Used train-test-split to split the data into 2 smaller files and saving them.

~ Final method was to call this class and execute all this code.

ALong with 'entity config', i made 'artificat config'; where i specified to get the path ofthe saved train and test files 
as string. I will use this later on as input to data validation.

I called this finally in ingestion later on in execution part.

6. main.py

outside the Retail folder, i made another file with the name main.py,
i imported all the relevant libraries there and wrote a code to execute the entire code ingestion pipeline.


